{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "ss = preprocessing.StandardScaler()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://db.netkeiba.com/race/\"\n",
    "# keibajou = {'01':'札幌', '02':'函館', '03':'福島', '04':'新潟', '05':'東京', '06':'中山', '07':'中京', '08':'京都', '09':'阪神', '10':'小倉'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''スクレイピング'''\n",
    "# メインテーブルのスクレイピング\n",
    "def scrape_netkeiba_results(year, course='全部', pre_race_results={}):\n",
    "    \n",
    "    #URLの番号部分１２桁の作成\n",
    "    race_course = {'札幌': 1, '函館': 2, '福島': 3, '新潟': 4, '東京': 5, '中山': 6, '中京': 7, '京都': 8, '阪神': 9, '小倉': 10, '全部': 11}\n",
    "    place = race_course[course]\n",
    "    race_id_list = []\n",
    "    #馬場の指定\n",
    "    if place != 11:\n",
    "        for kai in range(1, 7, 1):\n",
    "            for y in year:\n",
    "                for day in range(1, 13, 1):\n",
    "                    for r in range(1, 13, 1):\n",
    "                        race_id = str(y) + str(place).zfill(2) + str(kai).zfill(2) + str(day).zfill(2) + str(r).zfill(2)\n",
    "                        race_id_list.append(race_id)\n",
    "    else:\n",
    "        for place in range(1, 11, 1):\n",
    "            for kai in range(1, 7, 1):\n",
    "                for y in year:\n",
    "                    for day in range(1, 13, 1):\n",
    "                        for r in range(1, 13, 1):\n",
    "                            race_id = str(y) + str(place).zfill(2) + str(kai).zfill(2) + str(day).zfill(2) + str(r).zfill(2)\n",
    "                            race_id_list.append(race_id)\n",
    "    #スクレイピング開始                    \n",
    "    race_results = pre_race_results.copy()\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/race/' + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            #horse_id,jockey_idをスクレイピング\n",
    "            html = requests.get(url)\n",
    "            html.encoding = 'EUC-JP'\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "            horse_id_list = []\n",
    "            horse_a_list = soup.find(\n",
    "                'table', attrs = {'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile('^/horse')})\n",
    "            for a in horse_a_list:\n",
    "                horse_id = re.findall(r'\\d+', a['href'])\n",
    "                horse_id_list.append(horse_id[0])\n",
    "            jockey_id_list = []\n",
    "            jockey_a_list = soup.find(\n",
    "                'table', attrs = {'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile('^/jockey')})\n",
    "            for a in jockey_a_list:\n",
    "                jockey_id = re.findall(r'\\d+', a['href'])\n",
    "                jockey_id_list.append(jockey_id[0])    \n",
    "            \n",
    "            df['horse_id'] = horse_id_list\n",
    "            df['jockey_id'] = jockey_id_list\n",
    "            race_results[race_id] = df\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except:\n",
    "            break\n",
    "    #dataframeに整形\n",
    "    for key in race_results:\n",
    "        race_results[key].index = [key] * len(race_results[key])\n",
    "    results = pd.concat([race_results[key] for key in race_results], sort=False)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    file_name = '{0:%Y%m%d_%H%M%S}.pickle'.format(now)\n",
    "    results.to_pickle(file_name)\n",
    "    return results\n",
    "# 天気や馬場状態や日付を取得する\n",
    "def scrape_race_info(race_id_list):\n",
    "    race_infos = {}\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/race/' + str(race_id)\n",
    "            html = requests.get(url)\n",
    "            html.encoding = 'EUC-JP'\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "            texts = soup.find_all('p')[3].text + soup.find_all('p')[4].text\n",
    "            info = re.findall(r'\\w+', texts)\n",
    "\n",
    "            info_dict = {}\n",
    "            for text in info:\n",
    "                if text in ['芝', 'ダート']:\n",
    "                    info_dict['race_type'] = text\n",
    "                if '障' in text:\n",
    "                    info_dict['race_type'] = '障害'\n",
    "                if 'm' in text:\n",
    "                    info_dict['course_len'] = int(re.findall(r\"\\d+\", text)[0])\n",
    "                if text in ['良', '稍重', '重', '不良']:\n",
    "                    info_dict['ground_state'] = text\n",
    "                if text in ['曇', '晴', '雨', '小雨', '小雪', '雪']:\n",
    "                    info_dict['weather'] = text\n",
    "                if '年' in text:\n",
    "                    info_dict['date'] = text\n",
    "                if '歳' in text:\n",
    "                    info_dict['race_name'] = text\n",
    "\n",
    "            race_infos[race_id] = info_dict\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return race_infos\n",
    "\n",
    "\n",
    "# 払い戻し情報をスクレイピング\n",
    "def scrape_return_tables(race_id_list, pre_return_tables={}):\n",
    "    return_tables = pre_return_tables\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/race/' + race_id\n",
    "            \n",
    "            #普通にスクレイピングすると複勝やワイドなどが区切られないで繋がってしまう。\n",
    "            #そのため、改行コードを文字列brに変換して後でsplitする\n",
    "            f = urlopen(url)\n",
    "            html = f.read()\n",
    "            html = html.replace(b'<br />', b'br')\n",
    "            dfs = pd.read_html(html)\n",
    "\n",
    "            #dfsの1番目に単勝〜馬連、2番目にワイド〜三連単がある\n",
    "            df = pd.concat([dfs[1], dfs[2]])\n",
    "\n",
    "            df.index = [race_id] * len(df)\n",
    "            return_tables[race_id] = df\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e: #捕捉できるエラーは原因がわかるようにprintしてからbreak\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return return_tables\n",
    "# 血統データをスクレイピング\n",
    "def scrape_peds(horse_id_list, pre_peds={}):\n",
    "    peds = pre_peds\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/ped/' + horse_id\n",
    "            df = pd.read_html(url)[0]\n",
    "\n",
    "            generations = {}\n",
    "            for i in reversed(range(5)):\n",
    "                generations[i] = df[i]\n",
    "                df.drop([i], axis=1, inplace=True)\n",
    "                df = df.drop_duplicates()\n",
    "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "            peds[horse_id] = ped.reset_index(drop=True)\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return peds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''データクレンジング'''\n",
    "def pre_main(results):\n",
    "    df = results.copy()\n",
    "\n",
    "    df['race_id'] = df.index\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns='index', inplace=True)\n",
    "\n",
    "    df = df[~(df['着順'].astype(str).str.contains('\\D'))]\n",
    "    df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "    df['性'] = df['性齢'].map(lambda x: str(x)[0])\n",
    "    df = pd.get_dummies(data=df, columns=['性'], drop_first=True)\n",
    "    df['年齢'] = df['性齢'].map(lambda x: str(x)[1:]).astype(int)\n",
    "    df['体重'] = df['馬体重'].str.split('(', expand=True)[0].astype(int)\n",
    "    df['増減'] = df['馬体重'].str.split('(', expand=True)[1].str[:-1].astype(int)\n",
    "    df['単勝'] = df['単勝'].astype(float)\n",
    "    \n",
    "    df['rank'] = df['着順'].map(lambda x: 1 if x<4 else 0)\n",
    "    df.drop(['タイム', '着差', '性齢', '馬体重', '着順', '馬名', '騎手', '調教師', '人気', '単勝'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pre_info(infos):\n",
    "    df = infos.copy()\n",
    "\n",
    "    df['course_len'] = df['course_len'].astype(int)\n",
    "    df['race_id'] = df.index\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns='index', inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y年%m月%d日')\n",
    "    df = pd.get_dummies(data=df, columns=['weather', 'race_type', 'ground_state', 'race_name'], drop_first=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''モデル評価、収益計算'''\n",
    "class Return:\n",
    "    def __init__(self, return_tables):\n",
    "        self.return_tables = return_tables\n",
    "\n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1, 2]]\n",
    "        wins = fukusho[1].str.split('br', expand=True)[[0, 1, 2]]\n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "        returns = fukusho[2].str.split('br', expand=True)[[0, 1, 2]]\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "        return df.fillna(0).astype(int)\n",
    "    \n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0]=='単勝'][[1, 2]]\n",
    "        tansho.columns = ['win', 'return']\n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n",
    "        return tansho\n",
    "# return class内に入れるの検討↓\n",
    "def calculate_fukusho_return(df_y, fukusho, threshold=0.5):\n",
    "    pred_table = df_y.copy()[['race_id', '馬番', 'pred']]\n",
    "    fukusho_table = fukusho.copy()\n",
    "    pred_table['pred_threshold'] = pred_table['pred'].map(lambda x: 1 if x > threshold else 0)\n",
    "    pred_bet_table = pred_table[pred_table['pred_threshold']==1]\n",
    "    out_money = 100 * len(pred_bet_table)\n",
    "    fukusho_table.reset_index(inplace=True)\n",
    "    fukusho_table.columns = ['race_id', 'win_0', 'win_1', 'win_2', 'return_0', 'return_1', 'return_2']\n",
    "    df_calculate = pd.merge(pred_bet_table, fukusho_table, on='race_id', how='left')\n",
    "    in_money = 0\n",
    "    for i in range(3):\n",
    "        in_money += df_calculate[df_calculate['win_{}'.format(i)]==df_calculate['馬番']]['return_{}'.format(i)].sum()\n",
    "    return round(in_money/out_money, 3)*100, in_money-out_money\n",
    "\n",
    "\n",
    "'''運用フェーズ'''\n",
    "def update_data(old, new):\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old, new])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''運用フェーズ'''\n",
    "\n",
    "# 出馬データを作成\n",
    "class Shutuba:\n",
    "    def __init__(self, race_id_list):\n",
    "        self.race_id_list = race_id_list\n",
    "\n",
    "    def scrape(self):\n",
    "        data = pd.DataFrame()\n",
    "        for race_id in tqdm(self.race_id_list):\n",
    "            url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            df = df.T.reset_index(level=0, drop=True).T\n",
    "\n",
    "            html = requests.get(url)\n",
    "            html.encoding = 'EUC-JP'\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
    "            texts = re.findall(r'\\w+', texts)\n",
    "            for text in texts:\n",
    "                if 'm' in text:\n",
    "                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
    "                if text in ['曇', '晴', '雨', '小雨', '小雪', '雪']:\n",
    "                    df[\"weather\"] = [text] * len(df)\n",
    "                if text in ['良', '稍重', '重']:\n",
    "                    df['ground_state'] = [text] * len(df)\n",
    "                if '不' in text:\n",
    "                    df['ground_state'] = ['不良'] * len(df)\n",
    "                if '芝' in text:\n",
    "                    df['race_type'] = ['芝'] * len(df)\n",
    "                if '障' in text:\n",
    "                    df['race_type'] = ['障害'] * len(df)\n",
    "                if 'ダ' in text:\n",
    "                    df['race_type'] = ['ダート'] * len(df)\n",
    "            horse_id_list = []\n",
    "            horse_td_list = soup.find_all('td', attrs={'class': 'HorseInfo'})\n",
    "            for td in horse_td_list:\n",
    "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                horse_id_list.append(horse_id)\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_td_list = soup.find_all('td', attrs={'class': 'Jockey'})\n",
    "            for td in jockey_td_list:\n",
    "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                jockey_id_list.append(jockey_id)\n",
    "            df['horse_id'] = horse_id_list\n",
    "            df['jockey_id'] = jockey_id_list\n",
    "            df.index = [race_id] * len(df)\n",
    "            data = data.append(df)\n",
    "            time.sleep(1)\n",
    "            now = datetime.now()\n",
    "            file_name = 'shutuba_{0:%Y%m%d_%H%M%S}.pickle'.format(now)\n",
    "            data.to_pickle(file_name)\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n",
    "        df['年齢'] = df['性齢'].map(lambda x: str(x)[1:]).astype(int)\n",
    "        df['体重'] = df['馬体重'].str.split('(', expand=True)[0].astype(int)\n",
    "        df['増減'] = df['馬体重'].str.split('(', expand=True)[1].str[:-1].astype(int)\n",
    "        \n",
    "        \n",
    "        df = pd.get_dummies(data=df, columns=['性', 'race_type','weather', 'ground_state'], drop_first=True)\n",
    "        df.drop(columns=['印', '馬名', '騎手', '厩舎', 'Unnamed: 9_level_1', '人気', '登録', 'メモ'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクレイピングによるデータ収集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阪神競馬場10年分のmainデータをスクレイピング\n",
    "# year = [y for y in range(2012, 2022)]\n",
    "# results = scrape_netkeiba_results(year=year, course='阪神')\n",
    "\n",
    "# df = pd.read_pickle('2012-2021hanshin.pickle')\n",
    "# df['race_id'] = df.index\n",
    "# df.reset_index(inplace=True)\n",
    "# df.drop(columns='index', inplace=True)\n",
    "\n",
    "# race_id_list = list(df.race_id.unique())\n",
    "# horse_id_list = list(df.horse_id.unique())\n",
    "# print(len(horse_id_list))\n",
    "# print(len(race_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 天気などの情報をスクレイピング\n",
    "# dict_info = scrape_race_info(race_id_list)\n",
    "\n",
    "# df_info = pd.DataFrame(dict_info).T\n",
    "# df_info.to_pickle('df_info.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 競走馬情報をスクレイピング\n",
    "# スクレイピングの都合上、取得年を分ける\n",
    "\n",
    "# hr_id_list2014 = make_horse_id(2014, 2014)\n",
    "# dict_horse_2014 = scrape_horse_results(hr_id_list2014)\n",
    "# df_horse_2014 = pre_horse(dict_horse_2014)\n",
    "# df_horse_2014.to_pickle('df_horse_2014.pickle')\n",
    "# df_horse_2014 = pd.read_pickle('df_horse_2014')\n",
    "\n",
    "# hr_id_list2015 = make_horse_id(2015, 2015)\n",
    "# dict_horse_2015 = scrape_horse_results(hr_id_list2015)\n",
    "# df_horse_2015 = pre_horse(dict_horse_2015)\n",
    "# df_horse_2015.to_pickle('df_horse_2015.pickle')\n",
    "# df_horse_2015 = pd.read_pickle('df_horse_2015')\n",
    "\n",
    "# hr_id_list2016 = make_horse_id(2016, 2016)\n",
    "# dict_horse_2016 = scrape_horse_results(hr_id_list2016)\n",
    "# df_horse_2016 = pre_horse(dict_horse_2016)\n",
    "# df_horse_2016.to_pickle('df_horse_2016.pickle')\n",
    "# df_horse_2016 = pd.read_pickle('df_horse_2016')\n",
    "\n",
    "# hr_id_list2017 = make_horse_id(2017, 2017)\n",
    "# dict_horse_2017 = scrape_horse_results(hr_id_list2017)\n",
    "# df_horse_2017 = pre_horse(dict_horse_2017)\n",
    "# df_horse_2017.to_pickle('df_horse_2017.pickle')\n",
    "# df_horse_2017 = pd.read_pickle('df_horse_2017')\n",
    "\n",
    "# hr_id_list2018 = make_horse_id(2018, 2018)\n",
    "# dict_horse_2018 = scrape_horse_results(hr_id_list2018)\n",
    "# df_horse_2018 = pre_horse(dict_horse_2018)\n",
    "# df_horse_2018.to_pickle('df_horse_2018.pickle')\n",
    "# df_horse_2018 = pd.read_pickle('df_horse_2018')\n",
    "\n",
    "# hr_id_list2019 = make_horse_id(2019, 2019)\n",
    "# dict_horse_2019 = scrape_horse_results(hr_id_list2019)\n",
    "# df_horse_2019 = pre_horse(dict_horse_2019)\n",
    "# df_horse_2019.to_pickle('df_horse_2019.pickle')\n",
    "# df_horse_2019 = pd.read_pickle('df_horse_2019')\n",
    "\n",
    "# df_horse_alldata = pd.concat([df_horse_2014, df_horse_2015, df_horse_2016, df_horse_2017, df_horse_2018, df_horse_2019])\n",
    "# df_horse_alldata.to_pickle('df_horse_alldata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 払い戻し情報をスクレイピング\n",
    "# dict_return = scrape_return_tables(race_id_list)\n",
    "\n",
    "# dict_returnをデータフレーム型に変換しpickleファイルで保存\n",
    "# df_return = pd.DataFrame()\n",
    "# for i in tqdm(dict_return.keys()):\n",
    "#     dict_return[i]['race_id'] = i\n",
    "#     df_return = pd.concat([df_return, dict_return[i]])\n",
    "\n",
    "# df_return.to_pickle('df_return.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出馬表のスクレイピング\n",
    "# shutuba_table = scrape(df.race_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データクレンジング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "df = pd.read_pickle('2012-2021hanshin.pickle')\n",
    "df_info = pd.read_pickle('df_info.pickle')\n",
    "df_return = pd.read_pickle('df_return.pickle')\n",
    "df_horse_all = pd.read_pickle('df_horse_alldata.pickle')\n",
    "\n",
    "# 複勝データを使用\n",
    "rt = Return(df_return)\n",
    "df_fukusho = rt.fukusho\n",
    "\n",
    "# データの前処理\n",
    "df = pre_main(df)\n",
    "df_info = pre_info(df_info)\n",
    "df = pd.merge(df, df_info, on='race_id', how='left')\n",
    "df = pd.merge(df, df_horse_all, on='horse_id', how='left')\n",
    "df.jockey_id = df.jockey_id.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_columns = ['枠番', '馬番', '斤量', 'jockey_id', '性_牝', '性_牡', '年齢', '体重', '増減', \n",
    "       'course_len', 'weather_小雪', 'weather_晴', 'weather_曇', 'weather_雨', 'race_type_芝', 'race_type_障害',\n",
    "       'ground_state_稍重', 'ground_state_良', 'ground_state_重',\n",
    "       'race_name_2歳500万下', 'race_name_2歳オープン', 'race_name_2歳新馬',\n",
    "       'race_name_2歳未勝利', 'race_name_3歳1勝クラス', 'race_name_3歳500万下',\n",
    "       'race_name_3歳オープン', 'race_name_3歳以上1000万下', 'race_name_3歳以上1600万下',\n",
    "       'race_name_3歳以上1勝クラス', 'race_name_3歳以上2勝クラス', 'race_name_3歳以上3勝クラス',\n",
    "       'race_name_3歳以上500万下', 'race_name_3歳以上オープン', 'race_name_3歳新馬',\n",
    "       'race_name_3歳未勝利', 'race_name_4歳以上1000万下', 'race_name_4歳以上1600万下',\n",
    "       'race_name_4歳以上1勝クラス', 'race_name_4歳以上2勝クラス', 'race_name_4歳以上3勝クラス',\n",
    "       'race_name_4歳以上500万下', 'race_name_4歳以上オープン', 'race_name_障害3歳以上オープン',\n",
    "       'race_name_障害3歳以上未勝利', 'race_name_障害4歳以上オープン', 'race_name_障害4歳以上未勝利',\n",
    "       'ALLprize', 'ALLcounts', 'ALLrank', 'HANSHINprize', 'HANSHINcounts',\n",
    "       'HANSHINrank', 'SPLINTprize', 'SPLINTcounts', 'SPLINTrank', 'MILEprize',\n",
    "       'MILEcounts', 'MILErank', 'CLASSICprize', 'CLASSICcounts',\n",
    "       'CLASSICrank', 'STAYERprize', 'STAYERcounts', 'STAYERrank', 'TURFprize',\n",
    "       'TURFcounts', 'TURFrank', 'DIRTprize', 'DIRTcounts', 'DIRTrank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "train, test = df[df.date.dt.strftime('%Y')!='2021'], df[df.date.dt.strftime('%Y')=='2021']\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_x, train_t, test_x, test_t = train[select_columns], train['rank'], test[select_columns], test['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 閾値の設定\n",
    "threshold = 0.5\n",
    "\n",
    "# lightGBM\n",
    "model_lgbm = lgb.LGBMClassifier(random_state=58)\n",
    "model_lgbm.fit(train_x, train_t)\n",
    "lgbm_pred = model_lgbm.predict_proba(test_x)[:, 1]\n",
    "print(f'LightGMB score: {roc_auc_score(test_t, lgbm_pred)}')\n",
    "test['pred'] = lgbm_pred\n",
    "money1, money2 = calculate_fukusho_return(test, df_fukusho, threshold=threshold)\n",
    "print(f'回収率:{money1}%, 収支:{money2}円')\n",
    "print()\n",
    "\n",
    "# RandomForest\n",
    "model_rfc = RandomForestClassifier(n_jobs=-1, random_state=58)\n",
    "model_rfc.fit(train_x, train_t)\n",
    "rfc_pred = model_rfc.predict_proba(test_x)[:, 1]\n",
    "print(f'RandomForest score: {roc_auc_score(test_t, rfc_pred)}')\n",
    "test['pred'] = rfc_pred\n",
    "money1, money2 = calculate_fukusho_return(test, df_fukusho, threshold=threshold)\n",
    "print(f'回収率:{money1}%, 収支:{money2}円')\n",
    "print()\n",
    "\n",
    "# catboost\n",
    "model_cat = CatBoostClassifier(verbose=False, random_state=58)\n",
    "model_cat.fit(train_x, train_t)\n",
    "cat_pred = model_cat.predict_proba(test_x)[:, 1]\n",
    "print(f'CatBoost score: {roc_auc_score(test_t, cat_pred)}')\n",
    "test['pred'] = cat_pred\n",
    "money1, money2 = calculate_fukusho_return(test, df_fukusho, threshold=threshold)\n",
    "print(f'回収率:{money1}%, 収支:{money2}円')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出馬データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最新情報への更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2267c7f86544334f90fd828bd0d523f9563b963bbe28c7d2d8efd515b9a10ad6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.kaggle_')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
